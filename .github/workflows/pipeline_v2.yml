name: Pipeline v2 (meta → GPS → images)

on:
  schedule:
    - cron: '0 1 * * *'   # denne 01:00 UTC
  workflow_dispatch:

jobs:
  scrape-meta:
    name: 1) Scrape META → BQ + staging
    runs-on: ubuntu-latest
    concurrency:
      group: pipeline-v2-scrape-meta
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas openpyxl \
                      pandas-gbq google-cloud-bigquery pyarrow db-dtypes

      - name: Write service account key
        shell: bash
        run: echo "$GCP_SA_KEY" > sa.json
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

      # TU sa spúšťa nový scraper
      - name: Run scraper_meta.py (META v2)
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/sa.json
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET: ${{ secrets.BQ_DATASET_V2 || secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}
          BQ_ENABLE: ${{ secrets.BQ_ENABLE || 'true' }}
          SAVE_EXCEL: ${{ secrets.SAVE_EXCEL || 'false' }}
          WORKSPACE_HOME: ${{ github.workspace }}
        run: python scraper_meta.py

  gps-collector:
    name: 2) GPS collector → BQ
    runs-on: ubuntu-latest
    needs: scrape-meta
    concurrency:
      group: pipeline-v2-gps
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pandas-gbq google-cloud-bigquery pyarrow db-dtypes
          pip install playwright
          python -m playwright install --with-deps chromium

      - name: Write service account key
        shell: bash
        run: echo "$GCP_SA_KEY" > sa.json
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

      - name: Run gps_collector.py
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/sa.json
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET: ${{ secrets.BQ_DATASET_V2 || secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}
        run: python gps_collector.py

  images:
    name: 3) Images → GCS + BQ
    runs-on: ubuntu-latest
    needs: scrape-meta
    concurrency:
      group: pipeline-v2-images
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas google-cloud-storage google-cloud-bigquery pyarrow db-dtypes
      - name: Write service account key
        shell: bash
        run: echo "$GCP_SA_KEY" > sa.json
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
      - name: Run image_downloader.py
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/sa.json
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET: ${{ secrets.BQ_DATASET_V2 || secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
          # ---- limity natvrdo pre testovanie ----
          IMAGES_LOOKBACK_DAYS: '3'
          IMAGES_MAX_LISTINGS: '10'
          IMAGES_MAX_PER_LISTING: '60'
          # Jednorazovo necháš takto a potom vymažeš:
          # IMAGES_TABLE_RECREATE: 'true'
        run: python image_downloader.py

