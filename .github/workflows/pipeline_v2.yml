name: Pipeline v2 (meta → GPS → images)

on:
  schedule:
    - cron: '0 1 * * *'   # denne 01:00 UTC
  workflow_dispatch:

jobs:
  scrape-meta:
    name: 1) Scrape META → BQ + staging
    runs-on: ubuntu-latest
    concurrency:
      group: pipeline-v2-scrape-meta
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas openpyxl pandas-gbq google-cloud-bigquery pyarrow db-dtypes

      - name: Write service account key
        shell: bash
        run: echo "$GCP_SA_KEY" > sa.json
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

      # VARIANT A: spúšťame existujúci scraper.py (nie scraper_meta.py)
      - name: Run scraper.py (META)
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/sa.json
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          # preferuj nový dataset V2, inak padni na pôvodný
          BQ_DATASET: ${{ secrets.BQ_DATASET_V2 || secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}
          BQ_ENABLE: ${{ secrets.BQ_ENABLE || 'true' }}
          SAVE_EXCEL: ${{ secrets.SAVE_EXCEL || 'false' }}
          WORKSPACE_HOME: ${{ github.workspace }}
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
        run: python scraper.py

  gps-collector:
    name: 2) GPS collector → BQ
    runs-on: ubuntu-latest
    needs: scrape-meta
    concurrency:
      group: pipeline-v2-gps
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pandas-gbq google-cloud-bigquery pyarrow db-dtypes
          pip install playwright
          python -m playwright install --with-deps chromium

      - name: Write service account key
        shell: bash
        run: echo "$GCP_SA_KEY" > sa.json
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

      - name: Run gps_collector.py
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/sa.json
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          # zapisuj rovno do V2
          BQ_DATASET: ${{ secrets.BQ_DATASET_V2 || secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}
        run: python gps_collector.py

  images:
    name: 3) Images → GCS + BQ
    runs-on: ubuntu-latest
    needs: scrape-meta
    concurrency:
      group: pipeline-v2-images
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas pandas-gbq google-cloud-bigquery google-cloud-storage pyarrow db-dtypes

      - name: Write service account key
        shell: bash
        run: echo "$GCP_SA_KEY" > sa.json
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

      - name: Run image_downloader.py
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/sa.json
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          # zapisuj do V2
          BQ_DATASET: ${{ secrets.BQ_DATASET_V2 || secrets.BQ_DATASET }}
          BQ_LOCATION: ${{ secrets.BQ_LOCATION }}
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
        run: python image_downloader.py
